:: loading settings :: url = jar:file:/opt/bitnami/spark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
Ivy Default Cache set to: /opt/bitnami/spark/.ivy2/cache
The jars for the packages stored in: /opt/bitnami/spark/.ivy2/jars
org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-0dbe105e-61a0-4709-b2fd-e56b47368e24;1.0
	confs: [default]
	found org.apache.spark#spark-sql-kafka-0-10_2.12;3.1.1 in central
	found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.1.1 in central
	found org.apache.kafka#kafka-clients;2.6.0 in central
	found com.github.luben#zstd-jni;1.4.8-1 in central
	found org.lz4#lz4-java;1.7.1 in central
	found org.xerial.snappy#snappy-java;1.1.8.2 in central
	found org.slf4j#slf4j-api;1.7.30 in central
	found org.spark-project.spark#unused;1.0.0 in central
	found org.apache.commons#commons-pool2;2.6.2 in central
:: resolution report :: resolve 1336ms :: artifacts dl 78ms
	:: modules in use:
	com.github.luben#zstd-jni;1.4.8-1 from central in [default]
	org.apache.commons#commons-pool2;2.6.2 from central in [default]
	org.apache.kafka#kafka-clients;2.6.0 from central in [default]
	org.apache.spark#spark-sql-kafka-0-10_2.12;3.1.1 from central in [default]
	org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.1.1 from central in [default]
	org.lz4#lz4-java;1.7.1 from central in [default]
	org.slf4j#slf4j-api;1.7.30 from central in [default]
	org.spark-project.spark#unused;1.0.0 from central in [default]
	org.xerial.snappy#snappy-java;1.1.8.2 from central in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   9   |   0   |   0   |   0   ||   9   |   0   |
	---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent-0dbe105e-61a0-4709-b2fd-e56b47368e24
	confs: [default]
	0 artifacts copied, 9 already retrieved (0kB/25ms)
21/04/17 20:11:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
21/04/17 20:11:41 INFO SparkContext: Running Spark version 3.1.1
21/04/17 20:11:41 INFO ResourceUtils: ==============================================================
21/04/17 20:11:41 INFO ResourceUtils: No custom resources configured for spark.driver.
21/04/17 20:11:41 INFO ResourceUtils: ==============================================================
21/04/17 20:11:41 INFO SparkContext: Submitted application: stediRisk
21/04/17 20:11:41 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
21/04/17 20:11:41 INFO ResourceProfile: Limiting resource is cpu
21/04/17 20:11:41 INFO ResourceProfileManager: Added ResourceProfile id: 0
21/04/17 20:11:41 INFO SecurityManager: Changing view acls to: spark
21/04/17 20:11:41 INFO SecurityManager: Changing modify acls to: spark
21/04/17 20:11:41 INFO SecurityManager: Changing view acls groups to: 
21/04/17 20:11:41 INFO SecurityManager: Changing modify acls groups to: 
21/04/17 20:11:41 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(spark); groups with view permissions: Set(); users  with modify permissions: Set(spark); groups with modify permissions: Set()
21/04/17 20:11:42 INFO Utils: Successfully started service 'sparkDriver' on port 35467.
21/04/17 20:11:42 INFO SparkEnv: Registering MapOutputTracker
21/04/17 20:11:42 INFO SparkEnv: Registering BlockManagerMaster
21/04/17 20:11:42 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
21/04/17 20:11:42 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
21/04/17 20:11:42 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
21/04/17 20:11:42 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-b5bb0165-ac3b-4d55-9160-5dacc60c11ae
21/04/17 20:11:42 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
21/04/17 20:11:42 INFO SparkEnv: Registering OutputCommitCoordinator
21/04/17 20:11:42 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
21/04/17 20:11:42 INFO Utils: Successfully started service 'SparkUI' on port 4041.
21/04/17 20:11:43 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://817a9a05793c:4041
21/04/17 20:11:43 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.1.1.jar at spark://817a9a05793c:35467/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.1.1.jar with timestamp 1618690301595
21/04/17 20:11:43 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.1.1.jar at spark://817a9a05793c:35467/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.1.1.jar with timestamp 1618690301595
21/04/17 20:11:43 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-2.6.0.jar at spark://817a9a05793c:35467/jars/org.apache.kafka_kafka-clients-2.6.0.jar with timestamp 1618690301595
21/04/17 20:11:43 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.6.2.jar at spark://817a9a05793c:35467/jars/org.apache.commons_commons-pool2-2.6.2.jar with timestamp 1618690301595
21/04/17 20:11:43 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at spark://817a9a05793c:35467/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1618690301595
21/04/17 20:11:43 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/com.github.luben_zstd-jni-1.4.8-1.jar at spark://817a9a05793c:35467/jars/com.github.luben_zstd-jni-1.4.8-1.jar with timestamp 1618690301595
21/04/17 20:11:43 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.7.1.jar at spark://817a9a05793c:35467/jars/org.lz4_lz4-java-1.7.1.jar with timestamp 1618690301595
21/04/17 20:11:43 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.2.jar at spark://817a9a05793c:35467/jars/org.xerial.snappy_snappy-java-1.1.8.2.jar with timestamp 1618690301595
21/04/17 20:11:43 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-1.7.30.jar at spark://817a9a05793c:35467/jars/org.slf4j_slf4j-api-1.7.30.jar with timestamp 1618690301595
21/04/17 20:11:43 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.1.1.jar at file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.1.1.jar with timestamp 1618690301595
21/04/17 20:11:43 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.1.1.jar to /tmp/spark-f2f5f57a-0ecc-4643-af43-5b94c11d2baa/userFiles-6eab388d-7f31-4b72-8520-4b735f45afaf/org.apache.spark_spark-sql-kafka-0-10_2.12-3.1.1.jar
21/04/17 20:11:43 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.1.1.jar at file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.1.1.jar with timestamp 1618690301595
21/04/17 20:11:43 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.1.1.jar to /tmp/spark-f2f5f57a-0ecc-4643-af43-5b94c11d2baa/userFiles-6eab388d-7f31-4b72-8520-4b735f45afaf/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.1.1.jar
21/04/17 20:11:43 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-2.6.0.jar at file:///opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-2.6.0.jar with timestamp 1618690301595
21/04/17 20:11:43 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-2.6.0.jar to /tmp/spark-f2f5f57a-0ecc-4643-af43-5b94c11d2baa/userFiles-6eab388d-7f31-4b72-8520-4b735f45afaf/org.apache.kafka_kafka-clients-2.6.0.jar
21/04/17 20:11:43 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.6.2.jar at file:///opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.6.2.jar with timestamp 1618690301595
21/04/17 20:11:43 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.6.2.jar to /tmp/spark-f2f5f57a-0ecc-4643-af43-5b94c11d2baa/userFiles-6eab388d-7f31-4b72-8520-4b735f45afaf/org.apache.commons_commons-pool2-2.6.2.jar
21/04/17 20:11:43 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at file:///opt/bitnami/spark/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1618690301595
21/04/17 20:11:43 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-f2f5f57a-0ecc-4643-af43-5b94c11d2baa/userFiles-6eab388d-7f31-4b72-8520-4b735f45afaf/org.spark-project.spark_unused-1.0.0.jar
21/04/17 20:11:43 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/com.github.luben_zstd-jni-1.4.8-1.jar at file:///opt/bitnami/spark/.ivy2/jars/com.github.luben_zstd-jni-1.4.8-1.jar with timestamp 1618690301595
21/04/17 20:11:43 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/com.github.luben_zstd-jni-1.4.8-1.jar to /tmp/spark-f2f5f57a-0ecc-4643-af43-5b94c11d2baa/userFiles-6eab388d-7f31-4b72-8520-4b735f45afaf/com.github.luben_zstd-jni-1.4.8-1.jar
21/04/17 20:11:43 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.7.1.jar at file:///opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.7.1.jar with timestamp 1618690301595
21/04/17 20:11:43 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.7.1.jar to /tmp/spark-f2f5f57a-0ecc-4643-af43-5b94c11d2baa/userFiles-6eab388d-7f31-4b72-8520-4b735f45afaf/org.lz4_lz4-java-1.7.1.jar
21/04/17 20:11:43 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.2.jar at file:///opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.2.jar with timestamp 1618690301595
21/04/17 20:11:43 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.2.jar to /tmp/spark-f2f5f57a-0ecc-4643-af43-5b94c11d2baa/userFiles-6eab388d-7f31-4b72-8520-4b735f45afaf/org.xerial.snappy_snappy-java-1.1.8.2.jar
21/04/17 20:11:43 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-1.7.30.jar at file:///opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-1.7.30.jar with timestamp 1618690301595
21/04/17 20:11:43 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-1.7.30.jar to /tmp/spark-f2f5f57a-0ecc-4643-af43-5b94c11d2baa/userFiles-6eab388d-7f31-4b72-8520-4b735f45afaf/org.slf4j_slf4j-api-1.7.30.jar
21/04/17 20:11:43 INFO Executor: Starting executor ID driver on host 817a9a05793c
21/04/17 20:11:43 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.2.jar with timestamp 1618690301595
21/04/17 20:11:43 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.2.jar has been previously copied to /tmp/spark-f2f5f57a-0ecc-4643-af43-5b94c11d2baa/userFiles-6eab388d-7f31-4b72-8520-4b735f45afaf/org.xerial.snappy_snappy-java-1.1.8.2.jar
21/04/17 20:11:43 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.1.1.jar with timestamp 1618690301595
21/04/17 20:11:43 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.1.1.jar has been previously copied to /tmp/spark-f2f5f57a-0ecc-4643-af43-5b94c11d2baa/userFiles-6eab388d-7f31-4b72-8520-4b735f45afaf/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.1.1.jar
21/04/17 20:11:43 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/com.github.luben_zstd-jni-1.4.8-1.jar with timestamp 1618690301595
21/04/17 20:11:43 INFO Utils: /opt/bitnami/spark/.ivy2/jars/com.github.luben_zstd-jni-1.4.8-1.jar has been previously copied to /tmp/spark-f2f5f57a-0ecc-4643-af43-5b94c11d2baa/userFiles-6eab388d-7f31-4b72-8520-4b735f45afaf/com.github.luben_zstd-jni-1.4.8-1.jar
21/04/17 20:11:43 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.1.1.jar with timestamp 1618690301595
21/04/17 20:11:43 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.1.1.jar has been previously copied to /tmp/spark-f2f5f57a-0ecc-4643-af43-5b94c11d2baa/userFiles-6eab388d-7f31-4b72-8520-4b735f45afaf/org.apache.spark_spark-sql-kafka-0-10_2.12-3.1.1.jar
21/04/17 20:11:43 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.6.2.jar with timestamp 1618690301595
21/04/17 20:11:43 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.6.2.jar has been previously copied to /tmp/spark-f2f5f57a-0ecc-4643-af43-5b94c11d2baa/userFiles-6eab388d-7f31-4b72-8520-4b735f45afaf/org.apache.commons_commons-pool2-2.6.2.jar
21/04/17 20:11:43 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1618690301595
21/04/17 20:11:43 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar has been previously copied to /tmp/spark-f2f5f57a-0ecc-4643-af43-5b94c11d2baa/userFiles-6eab388d-7f31-4b72-8520-4b735f45afaf/org.spark-project.spark_unused-1.0.0.jar
21/04/17 20:11:43 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.7.1.jar with timestamp 1618690301595
21/04/17 20:11:43 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.7.1.jar has been previously copied to /tmp/spark-f2f5f57a-0ecc-4643-af43-5b94c11d2baa/userFiles-6eab388d-7f31-4b72-8520-4b735f45afaf/org.lz4_lz4-java-1.7.1.jar
21/04/17 20:11:43 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-2.6.0.jar with timestamp 1618690301595
21/04/17 20:11:43 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-2.6.0.jar has been previously copied to /tmp/spark-f2f5f57a-0ecc-4643-af43-5b94c11d2baa/userFiles-6eab388d-7f31-4b72-8520-4b735f45afaf/org.apache.kafka_kafka-clients-2.6.0.jar
21/04/17 20:11:43 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-1.7.30.jar with timestamp 1618690301595
21/04/17 20:11:43 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-1.7.30.jar has been previously copied to /tmp/spark-f2f5f57a-0ecc-4643-af43-5b94c11d2baa/userFiles-6eab388d-7f31-4b72-8520-4b735f45afaf/org.slf4j_slf4j-api-1.7.30.jar
21/04/17 20:11:43 INFO Executor: Fetching spark://817a9a05793c:35467/jars/org.apache.kafka_kafka-clients-2.6.0.jar with timestamp 1618690301595
21/04/17 20:11:43 INFO TransportClientFactory: Successfully created connection to 817a9a05793c/172.19.0.4:35467 after 46 ms (0 ms spent in bootstraps)
21/04/17 20:11:43 INFO Utils: Fetching spark://817a9a05793c:35467/jars/org.apache.kafka_kafka-clients-2.6.0.jar to /tmp/spark-f2f5f57a-0ecc-4643-af43-5b94c11d2baa/userFiles-6eab388d-7f31-4b72-8520-4b735f45afaf/fetchFileTemp9118760427761342388.tmp
21/04/17 20:11:43 INFO Utils: /tmp/spark-f2f5f57a-0ecc-4643-af43-5b94c11d2baa/userFiles-6eab388d-7f31-4b72-8520-4b735f45afaf/fetchFileTemp9118760427761342388.tmp has been previously copied to /tmp/spark-f2f5f57a-0ecc-4643-af43-5b94c11d2baa/userFiles-6eab388d-7f31-4b72-8520-4b735f45afaf/org.apache.kafka_kafka-clients-2.6.0.jar
21/04/17 20:11:43 INFO Executor: Adding file:/tmp/spark-f2f5f57a-0ecc-4643-af43-5b94c11d2baa/userFiles-6eab388d-7f31-4b72-8520-4b735f45afaf/org.apache.kafka_kafka-clients-2.6.0.jar to class loader
21/04/17 20:11:43 INFO Executor: Fetching spark://817a9a05793c:35467/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1618690301595
21/04/17 20:11:43 INFO Utils: Fetching spark://817a9a05793c:35467/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-f2f5f57a-0ecc-4643-af43-5b94c11d2baa/userFiles-6eab388d-7f31-4b72-8520-4b735f45afaf/fetchFileTemp6544686071797276520.tmp
21/04/17 20:11:43 INFO Utils: /tmp/spark-f2f5f57a-0ecc-4643-af43-5b94c11d2baa/userFiles-6eab388d-7f31-4b72-8520-4b735f45afaf/fetchFileTemp6544686071797276520.tmp has been previously copied to /tmp/spark-f2f5f57a-0ecc-4643-af43-5b94c11d2baa/userFiles-6eab388d-7f31-4b72-8520-4b735f45afaf/org.spark-project.spark_unused-1.0.0.jar
21/04/17 20:11:43 INFO Executor: Adding file:/tmp/spark-f2f5f57a-0ecc-4643-af43-5b94c11d2baa/userFiles-6eab388d-7f31-4b72-8520-4b735f45afaf/org.spark-project.spark_unused-1.0.0.jar to class loader
21/04/17 20:11:43 INFO Executor: Fetching spark://817a9a05793c:35467/jars/org.apache.commons_commons-pool2-2.6.2.jar with timestamp 1618690301595
21/04/17 20:11:43 INFO Utils: Fetching spark://817a9a05793c:35467/jars/org.apache.commons_commons-pool2-2.6.2.jar to /tmp/spark-f2f5f57a-0ecc-4643-af43-5b94c11d2baa/userFiles-6eab388d-7f31-4b72-8520-4b735f45afaf/fetchFileTemp4772482619229568828.tmp
21/04/17 20:11:43 INFO Utils: /tmp/spark-f2f5f57a-0ecc-4643-af43-5b94c11d2baa/userFiles-6eab388d-7f31-4b72-8520-4b735f45afaf/fetchFileTemp4772482619229568828.tmp has been previously copied to /tmp/spark-f2f5f57a-0ecc-4643-af43-5b94c11d2baa/userFiles-6eab388d-7f31-4b72-8520-4b735f45afaf/org.apache.commons_commons-pool2-2.6.2.jar
21/04/17 20:11:43 INFO Executor: Adding file:/tmp/spark-f2f5f57a-0ecc-4643-af43-5b94c11d2baa/userFiles-6eab388d-7f31-4b72-8520-4b735f45afaf/org.apache.commons_commons-pool2-2.6.2.jar to class loader
21/04/17 20:11:43 INFO Executor: Fetching spark://817a9a05793c:35467/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.1.1.jar with timestamp 1618690301595
21/04/17 20:11:43 INFO Utils: Fetching spark://817a9a05793c:35467/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.1.1.jar to /tmp/spark-f2f5f57a-0ecc-4643-af43-5b94c11d2baa/userFiles-6eab388d-7f31-4b72-8520-4b735f45afaf/fetchFileTemp6435422235368171956.tmp
21/04/17 20:11:43 INFO Utils: /tmp/spark-f2f5f57a-0ecc-4643-af43-5b94c11d2baa/userFiles-6eab388d-7f31-4b72-8520-4b735f45afaf/fetchFileTemp6435422235368171956.tmp has been previously copied to /tmp/spark-f2f5f57a-0ecc-4643-af43-5b94c11d2baa/userFiles-6eab388d-7f31-4b72-8520-4b735f45afaf/org.apache.spark_spark-sql-kafka-0-10_2.12-3.1.1.jar
21/04/17 20:11:43 INFO Executor: Adding file:/tmp/spark-f2f5f57a-0ecc-4643-af43-5b94c11d2baa/userFiles-6eab388d-7f31-4b72-8520-4b735f45afaf/org.apache.spark_spark-sql-kafka-0-10_2.12-3.1.1.jar to class loader
21/04/17 20:11:43 INFO Executor: Fetching spark://817a9a05793c:35467/jars/org.slf4j_slf4j-api-1.7.30.jar with timestamp 1618690301595
21/04/17 20:11:43 INFO Utils: Fetching spark://817a9a05793c:35467/jars/org.slf4j_slf4j-api-1.7.30.jar to /tmp/spark-f2f5f57a-0ecc-4643-af43-5b94c11d2baa/userFiles-6eab388d-7f31-4b72-8520-4b735f45afaf/fetchFileTemp8826142875418616572.tmp
21/04/17 20:11:43 INFO Utils: /tmp/spark-f2f5f57a-0ecc-4643-af43-5b94c11d2baa/userFiles-6eab388d-7f31-4b72-8520-4b735f45afaf/fetchFileTemp8826142875418616572.tmp has been previously copied to /tmp/spark-f2f5f57a-0ecc-4643-af43-5b94c11d2baa/userFiles-6eab388d-7f31-4b72-8520-4b735f45afaf/org.slf4j_slf4j-api-1.7.30.jar
21/04/17 20:11:43 INFO Executor: Adding file:/tmp/spark-f2f5f57a-0ecc-4643-af43-5b94c11d2baa/userFiles-6eab388d-7f31-4b72-8520-4b735f45afaf/org.slf4j_slf4j-api-1.7.30.jar to class loader
21/04/17 20:11:43 INFO Executor: Fetching spark://817a9a05793c:35467/jars/org.lz4_lz4-java-1.7.1.jar with timestamp 1618690301595
21/04/17 20:11:43 INFO Utils: Fetching spark://817a9a05793c:35467/jars/org.lz4_lz4-java-1.7.1.jar to /tmp/spark-f2f5f57a-0ecc-4643-af43-5b94c11d2baa/userFiles-6eab388d-7f31-4b72-8520-4b735f45afaf/fetchFileTemp5897183603749662197.tmp
21/04/17 20:11:43 INFO Utils: /tmp/spark-f2f5f57a-0ecc-4643-af43-5b94c11d2baa/userFiles-6eab388d-7f31-4b72-8520-4b735f45afaf/fetchFileTemp5897183603749662197.tmp has been previously copied to /tmp/spark-f2f5f57a-0ecc-4643-af43-5b94c11d2baa/userFiles-6eab388d-7f31-4b72-8520-4b735f45afaf/org.lz4_lz4-java-1.7.1.jar
21/04/17 20:11:43 INFO Executor: Adding file:/tmp/spark-f2f5f57a-0ecc-4643-af43-5b94c11d2baa/userFiles-6eab388d-7f31-4b72-8520-4b735f45afaf/org.lz4_lz4-java-1.7.1.jar to class loader
21/04/17 20:11:43 INFO Executor: Fetching spark://817a9a05793c:35467/jars/com.github.luben_zstd-jni-1.4.8-1.jar with timestamp 1618690301595
21/04/17 20:11:43 INFO Utils: Fetching spark://817a9a05793c:35467/jars/com.github.luben_zstd-jni-1.4.8-1.jar to /tmp/spark-f2f5f57a-0ecc-4643-af43-5b94c11d2baa/userFiles-6eab388d-7f31-4b72-8520-4b735f45afaf/fetchFileTemp3763146812845947204.tmp
21/04/17 20:11:43 INFO Utils: /tmp/spark-f2f5f57a-0ecc-4643-af43-5b94c11d2baa/userFiles-6eab388d-7f31-4b72-8520-4b735f45afaf/fetchFileTemp3763146812845947204.tmp has been previously copied to /tmp/spark-f2f5f57a-0ecc-4643-af43-5b94c11d2baa/userFiles-6eab388d-7f31-4b72-8520-4b735f45afaf/com.github.luben_zstd-jni-1.4.8-1.jar
21/04/17 20:11:43 INFO Executor: Adding file:/tmp/spark-f2f5f57a-0ecc-4643-af43-5b94c11d2baa/userFiles-6eab388d-7f31-4b72-8520-4b735f45afaf/com.github.luben_zstd-jni-1.4.8-1.jar to class loader
21/04/17 20:11:43 INFO Executor: Fetching spark://817a9a05793c:35467/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.1.1.jar with timestamp 1618690301595
21/04/17 20:11:43 INFO Utils: Fetching spark://817a9a05793c:35467/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.1.1.jar to /tmp/spark-f2f5f57a-0ecc-4643-af43-5b94c11d2baa/userFiles-6eab388d-7f31-4b72-8520-4b735f45afaf/fetchFileTemp8498335247129212592.tmp
21/04/17 20:11:43 INFO Utils: /tmp/spark-f2f5f57a-0ecc-4643-af43-5b94c11d2baa/userFiles-6eab388d-7f31-4b72-8520-4b735f45afaf/fetchFileTemp8498335247129212592.tmp has been previously copied to /tmp/spark-f2f5f57a-0ecc-4643-af43-5b94c11d2baa/userFiles-6eab388d-7f31-4b72-8520-4b735f45afaf/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.1.1.jar
21/04/17 20:11:43 INFO Executor: Adding file:/tmp/spark-f2f5f57a-0ecc-4643-af43-5b94c11d2baa/userFiles-6eab388d-7f31-4b72-8520-4b735f45afaf/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.1.1.jar to class loader
21/04/17 20:11:43 INFO Executor: Fetching spark://817a9a05793c:35467/jars/org.xerial.snappy_snappy-java-1.1.8.2.jar with timestamp 1618690301595
21/04/17 20:11:43 INFO Utils: Fetching spark://817a9a05793c:35467/jars/org.xerial.snappy_snappy-java-1.1.8.2.jar to /tmp/spark-f2f5f57a-0ecc-4643-af43-5b94c11d2baa/userFiles-6eab388d-7f31-4b72-8520-4b735f45afaf/fetchFileTemp4422737864995470365.tmp
21/04/17 20:11:44 INFO Utils: /tmp/spark-f2f5f57a-0ecc-4643-af43-5b94c11d2baa/userFiles-6eab388d-7f31-4b72-8520-4b735f45afaf/fetchFileTemp4422737864995470365.tmp has been previously copied to /tmp/spark-f2f5f57a-0ecc-4643-af43-5b94c11d2baa/userFiles-6eab388d-7f31-4b72-8520-4b735f45afaf/org.xerial.snappy_snappy-java-1.1.8.2.jar
21/04/17 20:11:44 INFO Executor: Adding file:/tmp/spark-f2f5f57a-0ecc-4643-af43-5b94c11d2baa/userFiles-6eab388d-7f31-4b72-8520-4b735f45afaf/org.xerial.snappy_snappy-java-1.1.8.2.jar to class loader
21/04/17 20:11:44 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35793.
21/04/17 20:11:44 INFO NettyBlockTransferService: Server created on 817a9a05793c:35793
21/04/17 20:11:44 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
21/04/17 20:11:44 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 817a9a05793c, 35793, None)
21/04/17 20:11:44 INFO BlockManagerMasterEndpoint: Registering block manager 817a9a05793c:35793 with 366.3 MiB RAM, BlockManagerId(driver, 817a9a05793c, 35793, None)
21/04/17 20:11:44 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 817a9a05793c, 35793, None)
21/04/17 20:11:44 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 817a9a05793c, 35793, None)
21/04/17 20:11:44 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/opt/bitnami/spark/spark-warehouse').
21/04/17 20:11:44 INFO SharedState: Warehouse path is 'file:/opt/bitnami/spark/spark-warehouse'.
21/04/17 20:11:50 WARN StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-2eda33c8-b19f-47f0-a070-cd76406d3f79. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
-------------------------------------------
Batch: 0
-------------------------------------------
+--------------------+-----+
|            customer|score|
+--------------------+-----+
|Neeraj.Khatib@tes...| -5.5|
|Santosh.Doshi@tes...|  3.0|
|Sarah.Fibonnaci@t...|-11.5|
|   Dan.Huey@test.com|  3.5|
|Sean.Anderson@tes...|-21.0|
|Angie.Phillips@te...| -5.0|
|Gail.Staples@test...| -1.5|
|Gail.Harris@test.com| -1.5|
|Jason.Anderson@te...| -5.0|
| Dan.Howard@test.com| -2.0|
|  Lyn.Davis@test.com| -3.0|
|Lyn.Staples@test.com| -3.0|
|Edward.Ahmed@test...| -1.0|
|Jason.Doshi@test.com| -2.5|
|Trevor.Jones@test...| -4.5|
|David.Sanchez@tes...|-15.0|
|Angie.Olson@test.com| -8.0|
|Frank.Anderson@te...| -0.5|
|Neeraj.Abram@test...|-26.0|
|  Dan.Clark@test.com|  4.0|
+--------------------+-----+
only showing top 20 rows

-------------------------------------------
Batch: 1
-------------------------------------------
+--------------------+-----+
|            customer|score|
+--------------------+-----+
|Neeraj.Khatib@tes...|  6.0|
|Santosh.Doshi@tes...| 16.5|
|Sarah.Fibonnaci@t...|-12.5|
+--------------------+-----+

-------------------------------------------
Batch: 2
-------------------------------------------
+-----------------+-----+
|         customer|score|
+-----------------+-----+
|Dan.Huey@test.com|-10.5|
+-----------------+-----+

-------------------------------------------
Batch: 3
-------------------------------------------
+--------------------+-----+
|            customer|score|
+--------------------+-----+
|Sean.Anderson@tes...| -6.5|
+--------------------+-----+

-------------------------------------------
Batch: 4
-------------------------------------------
+--------------------+-----+
|            customer|score|
+--------------------+-----+
|Angie.Phillips@te...| -2.0|
+--------------------+-----+

-------------------------------------------
Batch: 5
-------------------------------------------
+--------------------+-----+
|            customer|score|
+--------------------+-----+
|Gail.Staples@test...| -3.5|
+--------------------+-----+

-------------------------------------------
Batch: 6
-------------------------------------------
+--------------------+-----+
|            customer|score|
+--------------------+-----+
|Gail.Harris@test.com| -3.5|
+--------------------+-----+

-------------------------------------------
Batch: 7
-------------------------------------------
+--------------------+-----+
|            customer|score|
+--------------------+-----+
|Jason.Anderson@te...| -4.5|
+--------------------+-----+

-------------------------------------------
Batch: 8
-------------------------------------------
+-------------------+-----+
|           customer|score|
+-------------------+-----+
|Dan.Howard@test.com| -2.5|
+-------------------+-----+

-------------------------------------------
Batch: 9
-------------------------------------------
+------------------+-----+
|          customer|score|
+------------------+-----+
|Lyn.Davis@test.com| -2.0|
+------------------+-----+

-------------------------------------------
Batch: 10
-------------------------------------------
+--------------------+-----+
|            customer|score|
+--------------------+-----+
|Lyn.Staples@test.com| -4.5|
+--------------------+-----+

-------------------------------------------
Batch: 11
-------------------------------------------
+--------------------+-----+
|            customer|score|
+--------------------+-----+
|Edward.Ahmed@test...| -1.5|
+--------------------+-----+

-------------------------------------------
Batch: 12
-------------------------------------------
+--------------------+-----+
|            customer|score|
+--------------------+-----+
|Jason.Doshi@test.com| -2.0|
+--------------------+-----+

-------------------------------------------
Batch: 13
-------------------------------------------
+--------------------+-----+
|            customer|score|
+--------------------+-----+
|Trevor.Jones@test...| -1.0|
+--------------------+-----+

-------------------------------------------
Batch: 14
-------------------------------------------
+--------------------+-----+
|            customer|score|
+--------------------+-----+
|David.Sanchez@tes...|-32.0|
+--------------------+-----+

-------------------------------------------
Batch: 15
-------------------------------------------
+--------------------+-----+
|            customer|score|
+--------------------+-----+
|Angie.Olson@test.com|  6.0|
+--------------------+-----+

-------------------------------------------
Batch: 16
-------------------------------------------
+--------------------+-----+
|            customer|score|
+--------------------+-----+
|Frank.Anderson@te...|-13.5|
+--------------------+-----+

-------------------------------------------
Batch: 17
-------------------------------------------
+--------------------+-----+
|            customer|score|
+--------------------+-----+
|Neeraj.Abram@test...| 16.5|
+--------------------+-----+

-------------------------------------------
Batch: 18
-------------------------------------------
+------------------+-----+
|          customer|score|
+------------------+-----+
|Dan.Clark@test.com| -3.0|
+------------------+-----+

-------------------------------------------
Batch: 19
-------------------------------------------
+--------------------+-----+
|            customer|score|
+--------------------+-----+
|Travis.Ahmed@test...|-39.5|
+--------------------+-----+

-------------------------------------------
Batch: 20
-------------------------------------------
+--------------------+-----+
|            customer|score|
+--------------------+-----+
|Trevor.Lincoln@te...| 22.5|
+--------------------+-----+

-------------------------------------------
Batch: 21
-------------------------------------------
+--------------------+-----+
|            customer|score|
+--------------------+-----+
|Jerry.Smith@test.com|-15.5|
+--------------------+-----+

-------------------------------------------
Batch: 22
-------------------------------------------
+--------------------+-----+
|            customer|score|
+--------------------+-----+
|Frank.Davis@test.com| 12.0|
+--------------------+-----+

-------------------------------------------
Batch: 23
-------------------------------------------
+--------------------+-----+
|            customer|score|
+--------------------+-----+
|Jerry.Aristotle@t...| 15.5|
+--------------------+-----+

^CTraceback (most recent call last):
  File "/home/workspace/sparkpyeventskafkastreamtoconsole.py", line 67, in <module>
21/04/17 20:12:46 ERROR WriteToDataSourceV2Exec: Data source write support org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@43c69784 is aborting.
21/04/17 20:12:46 ERROR WriteToDataSourceV2Exec: Data source write support org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@43c69784 aborted.
    "append").format("console").start().awaitTermination()
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming.py", line 101, in awaitTermination
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py", line 1303, in __call__
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py", line 1033, in send_command
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py", line 1200, in send_command
  File "/opt/bitnami/python/lib/python3.6/socket.py", line 586, in readinto
    return self._sock.recv_into(b)
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/context.py", line 285, in signal_handler
KeyboardInterrupt
21/04/17 20:12:46 ERROR MicroBatchExecution: Query [id = 46049036-a9c5-4ff9-a58e-78dfdd583b23, runId = f992f4b4-fc95-49e2-8239-3ca7285c2635] terminated with error
org.apache.spark.SparkException: Writing job aborted.
	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:388)
	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:336)
	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(WriteToDataSourceV2Exec.scala:297)
	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(WriteToDataSourceV2Exec.scala:304)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:40)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:40)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:46)
	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)
	at org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:2965)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)
	at org.apache.spark.sql.Dataset.collect(Dataset.scala:2965)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:589)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$15(MicroBatchExecution.scala:584)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:357)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:355)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:584)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:226)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:357)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:355)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:194)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:57)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:188)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:333)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:244)
Caused by: org.apache.spark.SparkException: Job 24 cancelled as part of cancellation of all jobs
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)
	at org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2149)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:971)
	at scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)
	at scala.collection.mutable.HashSet.foreach(HashSet.scala:79)
	at org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:970)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2405)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)
	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:357)
	... 37 more
21/04/17 20:12:47 ERROR Utils: Aborting task
org.apache.spark.TaskKilledException
	at org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:156)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:36)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:413)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:452)
	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:360)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
21/04/17 20:12:47 ERROR DataWritingSparkTask: Aborting commit for partition 0 (task 24, attempt 0, stage 24.0)
21/04/17 20:12:47 ERROR DataWritingSparkTask: Aborted commit for partition 0 (task 24, attempt 0, stage 24.0)
21/04/17 20:12:47 ERROR TaskContextImpl: Error in TaskCompletionListener
java.lang.IllegalStateException: Block broadcast_24 not found
	at org.apache.spark.storage.BlockInfoManager.$anonfun$unlock$3(BlockInfoManager.scala:293)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.storage.BlockInfoManager.unlock(BlockInfoManager.scala:293)
	at org.apache.spark.storage.BlockManager.releaseLock(BlockManager.scala:1196)
	at org.apache.spark.broadcast.TorrentBroadcast.$anonfun$releaseBlockManagerLock$1(TorrentBroadcast.scala:287)
	at org.apache.spark.broadcast.TorrentBroadcast.$anonfun$releaseBlockManagerLock$1$adapted(TorrentBroadcast.scala:287)
	at org.apache.spark.TaskContext$$anon$1.onTaskCompletion(TaskContext.scala:125)
	at org.apache.spark.TaskContextImpl.$anonfun$markTaskCompleted$1(TaskContextImpl.scala:124)
	at org.apache.spark.TaskContextImpl.$anonfun$markTaskCompleted$1$adapted(TaskContextImpl.scala:124)
	at org.apache.spark.TaskContextImpl.$anonfun$invokeListeners$1(TaskContextImpl.scala:137)
	at org.apache.spark.TaskContextImpl.$anonfun$invokeListeners$1$adapted(TaskContextImpl.scala:135)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:135)
	at org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:124)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
21/04/17 20:12:47 ERROR Utils: Uncaught exception in thread Executor task launch worker for task 0.0 in stage 24.0 (TID 24)
java.lang.NullPointerException
	at org.apache.spark.scheduler.Task.$anonfun$run$2(Task.scala:152)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1419)
	at org.apache.spark.scheduler.Task.run(Task.scala:150)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
:: loading settings :: url = jar:file:/opt/bitnami/spark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
Ivy Default Cache set to: /opt/bitnami/spark/.ivy2/cache
The jars for the packages stored in: /opt/bitnami/spark/.ivy2/jars
org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-4bf470fe-b6ca-4b96-a648-e74969461eb4;1.0
	confs: [default]
	found org.apache.spark#spark-sql-kafka-0-10_2.12;3.1.1 in central
	found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.1.1 in central
	found org.apache.kafka#kafka-clients;2.6.0 in central
	found com.github.luben#zstd-jni;1.4.8-1 in central
	found org.lz4#lz4-java;1.7.1 in central
	found org.xerial.snappy#snappy-java;1.1.8.2 in central
	found org.slf4j#slf4j-api;1.7.30 in central
	found org.spark-project.spark#unused;1.0.0 in central
	found org.apache.commons#commons-pool2;2.6.2 in central
:: resolution report :: resolve 990ms :: artifacts dl 61ms
	:: modules in use:
	com.github.luben#zstd-jni;1.4.8-1 from central in [default]
	org.apache.commons#commons-pool2;2.6.2 from central in [default]
	org.apache.kafka#kafka-clients;2.6.0 from central in [default]
	org.apache.spark#spark-sql-kafka-0-10_2.12;3.1.1 from central in [default]
	org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.1.1 from central in [default]
	org.lz4#lz4-java;1.7.1 from central in [default]
	org.slf4j#slf4j-api;1.7.30 from central in [default]
	org.spark-project.spark#unused;1.0.0 from central in [default]
	org.xerial.snappy#snappy-java;1.1.8.2 from central in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   9   |   0   |   0   |   0   ||   9   |   0   |
	---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent-4bf470fe-b6ca-4b96-a648-e74969461eb4
	confs: [default]
	0 artifacts copied, 9 already retrieved (0kB/55ms)
21/04/17 20:14:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
^C